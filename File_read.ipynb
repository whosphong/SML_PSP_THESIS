{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eadbd8a",
   "metadata": {},
   "source": [
    "# This code is used to extract and manipulate raw L2 + L3 cdf files into a nice CSV file contatining V0, V1, Vperp, f(v), and particle counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31689567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is in extFunc but I've copy and pasted it here so you [Phong] can see what it is\n",
    "#this is how you turn counts to EFLUX to VDF\n",
    "\n",
    "def createVDF(cdfData):\n",
    "\n",
    "    '''\n",
    "    Takes a CDF file object and creates an velocity distribution function in Vx, Vy, Vz from EFFLUX data.\n",
    "\n",
    "    This function is partially built on code found within tutorials found online for reading PSP data.\n",
    "\n",
    "    https://github.com/jlverniero/PSP_Data_Analysis_Tutorials/blob/main/README.md\n",
    "    '''\n",
    "\n",
    "    #0. Define useful constants and file information\n",
    "    qFlagKey = np.array([\n",
    "            'Bit0: Counter Overflow',\n",
    "            'Bit1: Survey Snapshot ON',\n",
    "            'Bit2: Alternate Energy Table',\n",
    "            'Bit3: Spoiler Test',\n",
    "            'Bit4: Attenuator Engaged',\n",
    "            'Bit5: Highest Archive Rate',\n",
    "            'Bit6: No Targeted Sweep',\n",
    "            'Bit7: SPAN-Ion New Mass Table',\n",
    "            'Bit8: Over-deflection',\n",
    "            'Bit9: Archive Snapshot ON',\n",
    "            'Bit10: Bad energy table',\n",
    "            'Bit11: MCP Test',\n",
    "            'Bit12: Survey available',\n",
    "            'Bit13: Archive available',\n",
    "            'Bit14: RESERVED',\n",
    "            'Bit15: RESERVED'])\n",
    "\n",
    "    mass_p = 0.01043970 #proton mass\n",
    "                        #[eV/(km/s)^2]\n",
    "                        #(938.272 * 10**6) / (299792.458**2)\n",
    "\n",
    "    #1. Read in useful information from file to np.ndarrays\n",
    "    numEntries = len(cdfData['TIME'])\n",
    "\n",
    "    eFlux      = cdfData['EFlux'].reshape((numEntries, 8, 32, 8)) # [eV/cm2-s-ster-eV]\n",
    "    counts     = cdfData['DATA'].reshape((numEntries, 8, 32, 8)) # [#]\n",
    "    time_accum = cdfData['TIME_ACCUM']\n",
    "\n",
    "\n",
    "    gFactor = (counts / time_accum[:, np.newaxis, np.newaxis, np.newaxis]) / eFlux #counts/time_accum/eflux -- Counts per second / eflux\n",
    "\n",
    "\n",
    "    #coords\n",
    "    #organized as an 8 phi-direction, 32 energy bins, and 8 theta-direction bins\n",
    "    energy = cdfData['ENERGY'].reshape((numEntries, 8, 32, 8)) # [eV]\n",
    "    theta  = cdfData['THETA'].reshape((numEntries, 8, 32, 8))  # [degree]\n",
    "    phi    = cdfData['PHI'].reshape((numEntries, 8, 32, 8))    # [degree]\n",
    "\n",
    "\n",
    "    #Calculate the one count level in vdf.\n",
    "    #we assume that when we can not calculate the gFactor due to restrictions in\n",
    "    #the access to public information, a reasonable assumption is that\n",
    "    #the gFactor is roughly similar to the mean.\n",
    "    #This is currently an unconstrained uncertainty.\n",
    "\n",
    "    fillVals = np.nanmean(gFactor, axis = (1, 2, 3))#np.nanmean(gfactor_day, axis = (1, 2, 3))\n",
    "\n",
    "    for day_i in np.arange(0, np.shape(gFactor)[0]):\n",
    "        #this is slow; future implementations should focus on speed ups through this\n",
    "        #avenue\n",
    "        gFactor[day_i, :, :, :] = np.nan_to_num(gFactor[day_i, :, :, :], nan = fillVals[day_i], copy = False)\n",
    "\n",
    "    eFlux_one_count = (1 / time_accum[:, np.newaxis, np.newaxis, np.newaxis]) / gFactor\n",
    "\n",
    "    #handle quality flags by returns an (nTime, 16) array\n",
    "    #of all active quality flags or nan (if unactive)\n",
    "    qualityFlags    = np.unpackbits(cdfData['QUALITY_FLAG'].view(np.uint8), bitorder = 'little')\n",
    "    qFlags          = qualityFlags.reshape(len(cdfData['QUALITY_FLAG']), 16).astype(np.int32)\n",
    "\n",
    "    reshaped_qFlags = np.tile(qFlagKey, len(cdfData['TIME'])).reshape(len(cdfData['TIME']), 16)\n",
    "    reshaped_qFlags[qFlags == 0] = np.nan\n",
    "\n",
    "    #2. Calculate velocity distriubtion function\n",
    "    #note; a derivation of the unit conversion from number flux to VDF\n",
    "    #is provided in G. Hanley's 2023 PhD Thesis\n",
    "\n",
    "    numberFlux = eFlux / energy # [#/cm2-s-ster-eV]\n",
    "\n",
    "    numberFlux_one_count = eFlux_one_count / energy\n",
    "\n",
    "    vdf_temp   = numberFlux*(mass_p**2)/((2 * 10**5)*energy) # [#/cm^3 (km/s)^3]\n",
    "\n",
    "    vdf_m      = vdf_temp * (100**3) #[# / m^3 (km/s)^3]\n",
    "\n",
    "    vdf_dm     = vdf_temp * (10**3) #[# / dm^3 (km/s)^3], 1 decimeter = 10 cm\n",
    "\n",
    "    vdf        = vdf_m\n",
    "\n",
    "    #one count\n",
    "\n",
    "    vdf_temp_one_count   = numberFlux_one_count*(mass_p**2)/((2 * 10**5)*energy) # [#/cm^3 (km/s)^3]\n",
    "\n",
    "    vdf_m_one_count      = vdf_temp_one_count * (100**3) #[# / m^3 (km/s)^3]\n",
    "\n",
    "    vdf_dm_one_count     = vdf_temp_one_count * (10**3) #[# / dm^3 (km/s)^3], 1 decimeter = 10 cm\n",
    "\n",
    "    vdf_one_count        = vdf_m_one_count\n",
    "\n",
    "    #Convert to velocity units in each energy channel\n",
    "    vel = np.sqrt(2*energy/mass_p) # [km/s]\n",
    "\n",
    "    #rotate from spherical to Cartesian\n",
    "    vx = vel * np.cos(np.radians(phi)) * np.cos(np.radians(theta))\n",
    "    vy = vel * np.sin(np.radians(phi)) * np.cos(np.radians(theta))\n",
    "    vz = vel *                           np.sin(np.radians(theta))\n",
    "\n",
    "    return(reshaped_qFlags, vdf, vdf_one_count, counts, vx, vy, vz)\n",
    "\n",
    "\n",
    "def calcRMatrix(L3Data):\n",
    "    '''\n",
    "    Estimates the rotation matrix from L3 data from instrument frame to a magnetic field coordinate frame.\n",
    "    '''\n",
    "\n",
    "    B = L3Data['MAGF_INST']\n",
    "\n",
    "    B_mag = np.linalg.norm(B, axis = 1, keepdims = True)\n",
    "\n",
    "    j_prime = B / B_mag\n",
    "\n",
    "\n",
    "    #find the least directional axes\n",
    "    #set this new axis to be the temp cross direction\n",
    "\n",
    "    indexMin = (j_prime == np.min(j_prime, 1, keepdims = True))\n",
    "\n",
    "    j = np.zeros(np.shape(j_prime))\n",
    "\n",
    "    j[indexMin] = 1\n",
    "\n",
    "    i_prime_unnormed = np.cross(j_prime, j)\n",
    "\n",
    "    i_prime = i_prime_unnormed / np.linalg.norm(i_prime_unnormed, axis = 1, keepdims = True)\n",
    "\n",
    "    k_prime_unnormed = np.cross(i_prime, j_prime)\n",
    "\n",
    "    k_prime = k_prime_unnormed / np.linalg.norm(k_prime_unnormed, axis = 1, keepdims = True)\n",
    "\n",
    "    A = np.stack([i_prime, j_prime, k_prime], 1)\n",
    "\n",
    "    return(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9db8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdflib\n",
    "import pandas                   as     pd\n",
    "import numpy                    as     np\n",
    "\n",
    "R_sun   = 695700000 # [meters]\n",
    "R_max   = 40        # [Solar radii]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9965da9",
   "metadata": {},
   "source": [
    "# Enter your files here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c38b0af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phong\\AppData\\Local\\Temp\\ipykernel_5656\\368274841.py:45: RuntimeWarning: invalid value encountered in divide\n",
      "  gFactor = (counts / time_accum[:, np.newaxis, np.newaxis, np.newaxis]) / eFlux #counts/time_accum/eflux -- Counts per second / eflux\n"
     ]
    }
   ],
   "source": [
    "L2File = r'C:/Users/Phong/OneDrive/Documents/VSCODEFILE/PSP_SML/SML_PSP_THESIS/PSP_RAW_DATA/psp_swp_spi_sf00_L2_8Dx32Ex8A_20231231_v04.cdf' ### ENTER YOUR L2 FILE PATH HERE\n",
    "L3File = r'C:/Users/Phong/OneDrive/Documents/VSCODEFILE/PSP_SML/SML_PSP_THESIS/PSP_RAW_DATA/psp_swp_spi_sf00_L3_mom_20231231_v04.cdf' ### ENTER YOUR L3 FILE PATH HERE\n",
    "\n",
    "L2Data = cdflib.CDF(L2File)\n",
    "L3Data = cdflib.CDF(L3File)\n",
    "\n",
    "qFlags_day, vdf_day, vdf_one_count_day, counts_day, vx_day, vy_day, vz_day = createVDF(L2Data)\n",
    "\n",
    "A_day = calcRMatrix(L3Data)\n",
    "\n",
    "times = pd.to_datetime(L2Data['TIME'], unit = 's')\n",
    "\n",
    "eligibleFilter = (L3Data['SUN_DIST'] / (R_sun/10**3)) < R_max\n",
    "\n",
    "#look at ONE vdf and not a DAY's VDF values, let's say the first one\n",
    "#in an eligible filter range?\n",
    "\n",
    "i = 2\n",
    "\n",
    "r_dist = L3Data['SUN_DIST'][eligibleFilter][i] / (R_sun/10**3)\n",
    "\n",
    "qFlags, vdf, vdf_one_count, vx, vy, vz, counts = qFlags_day[eligibleFilter][i], vdf_day[eligibleFilter][i], vdf_one_count_day[eligibleFilter][i], vx_day[eligibleFilter][i], vy_day[eligibleFilter][i], vz_day[eligibleFilter][i], counts_day[eligibleFilter][i]\n",
    "\n",
    "counts[:, [0], :] = 0\n",
    "vdf[:,    [0], :] = 0\n",
    "\n",
    "vdfDate = pd.to_datetime(L2Data['TIME'][eligibleFilter][i], unit = 's')\n",
    "\n",
    "A = A_day[i]\n",
    "\n",
    "v_perp_0, v_para, v_perp_1 = A @ np.array([vx.ravel(), vy.ravel(), vz.ravel()])\n",
    "\n",
    "v_perp_0 = np.reshape(v_perp_0, np.shape(vx))\n",
    "\n",
    "v_para   = np.reshape(v_para, np.shape(vx))\n",
    "\n",
    "v_perp_1 = np.reshape(v_perp_1, np.shape(vx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "635056c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"v_para\":   v_para.ravel(),\n",
    "    \"v_perp_0\": v_perp_0.ravel(),\n",
    "    \"v_perp_1\": v_perp_1.ravel(),\n",
    "    \"f(v)\":     vdf.ravel(),\n",
    "    \"counts\":   counts.ravel()\n",
    "})\n",
    "\n",
    "timestamp = pd.to_datetime(L2Data['TIME'][eligibleFilter][i], unit='s')\n",
    "timestamp_str = timestamp.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "filename = f\"vdf_{timestamp_str}.csv\"\n",
    "\n",
    "df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64e428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
